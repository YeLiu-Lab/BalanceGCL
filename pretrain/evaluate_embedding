import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import os

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, train_test_split
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import preprocessing
from sklearn.metrics import accuracy_score, f1_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

def draw_plot(datadir, DS, embeddings, fname, max_nodes=None):
    return
    graphs = read_graphfile(datadir, DS, max_nodes=max_nodes)
    labels = [graph.graph['label'] for graph in graphs]

    labels = preprocessing.LabelEncoder().fit_transform(labels)
    x, y = np.array(embeddings), np.array(labels)
    print('fitting TSNE ...')
    x = TSNE(n_components=2).fit_transform(x)

    plt.close()
    df = pd.DataFrame(columns=['x0', 'x1', 'Y'])

    df['x0'], df['x1'], df['Y'] = x[:,0], x[:,1], y
    sns.pairplot(x_vars=['x0'], y_vars=['x1'], data=df, hue="Y", size=5)
    plt.legend()
    plt.savefig(fname)

class LogReg(nn.Module):
    def __init__(self, ft_in, nb_classes):
        super(LogReg, self).__init__()
        self.fc = nn.Linear(ft_in, nb_classes)

        for m in self.modules():
            self.weights_init(m)

    def weights_init(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)

    def forward(self, seq):
        ret = self.fc(seq)
        return ret

def logistic_classify(x, y):

    nb_classes = np.unique(y).shape[0]
    xent = nn.CrossEntropyLoss()
    hid_units = x.shape[1]

    accs = []
    accs_val = []
    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)
    for train_index, test_index in kf.split(x, y):

        # test
        train_embs, test_embs = x[train_index], x[test_index]
        train_lbls, test_lbls= y[train_index], y[test_index]

        train_embs, train_lbls = torch.from_numpy(train_embs).cuda(), torch.from_numpy(train_lbls).cuda()
        test_embs, test_lbls= torch.from_numpy(test_embs).cuda(), torch.from_numpy(test_lbls).cuda()


        log = LogReg(hid_units, nb_classes)
        log.cuda()
        opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)

        best_val = 0
        test_acc = None
        for it in range(100):
            log.train()
            opt.zero_grad()

            logits = log(train_embs)
            loss = xent(logits, train_lbls)

            loss.backward()
            opt.step()

        logits = log(test_embs)
        preds = torch.argmax(logits, dim=1)
        acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]
        accs.append(acc.item())

        # val
        val_size = len(test_index)
        test_index = np.random.choice(test_index, val_size, replace=False).tolist()
        train_index = [i for i in train_index if not i in test_index]

        train_embs, test_embs = x[train_index], x[test_index]
        train_lbls, test_lbls= y[train_index], y[test_index]

        train_embs, train_lbls = torch.from_numpy(train_embs).cuda(), torch.from_numpy(train_lbls).cuda()
        test_embs, test_lbls= torch.from_numpy(test_embs).cuda(), torch.from_numpy(test_lbls).cuda()


        log = LogReg(hid_units, nb_classes)
        log.cuda()
        opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)

        best_val = 0
        test_acc = None
        for it in range(100):
            log.train()
            opt.zero_grad()

            logits = log(train_embs)
            loss = xent(logits, train_lbls)

            loss.backward()
            opt.step()

        logits = log(test_embs)
        preds = torch.argmax(logits, dim=1)
        acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]
        accs_val.append(acc.item())

    return np.mean(accs_val), np.mean(accs)

def svc_classify(x, y, search):
    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)
    accuracies = []
    accuracies_val = []
    for train_index, test_index in kf.split(x, y):

        # test
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)
        if search:
            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}
            classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=1)
        else:
            classifier = SVC(C=10)
        classifier.fit(x_train, y_train)
        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))

        # val
        val_size = len(test_index)
        test_index = np.random.choice(train_index, val_size, replace=False).tolist()
        train_index = [i for i in train_index if not i in test_index]

        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)
        if search:
            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}
            classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)
        else:
            classifier = SVC(C=10)
        classifier.fit(x_train, y_train)
        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))

    return np.mean(accuracies_val), np.mean(accuracies)

# def svc_classify(x, y, search):
#     kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)
#     accuracies = []
#     f1mis = []
#     f1mas = []
#     for train_index, test_index in kf.split(x, y):
#
#         # test
#         x_train, x_test = x[train_index], x[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)
#         if search:
#             params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}
#             classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)
#         else:
#             classifier = SVC(C=10)
#         classifier.fit(x_train, y_train)
#         accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))
#         f1mis.append(f1_score(y_test, classifier.predict(x_test), average="micro"))
#         f1mas.append(f1_score(y_test, classifier.predict(x_test), average="macro"))
#
#     return f1mis, f1mas


def svc_classify_ptc(x, x_aug, y, search):
    percentages = range(10, 100, 10)  # 从10%到90%的测试集比例
    results = []  # 存储每个比例下的结果

    for pct in percentages:
        accuracies = []
        accuracies_aug = []
        accuracies_same = []

        # 做10次取平均
        for _ in range(10):
            test_size = pct / 100  # 测试集所占的比例

            # # 分层划分训练集和测试集
            # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y)
            x_train, x_test, x_aug_train, x_aug_test, y_train, y_test = train_test_split(x, x_aug, y, test_size=test_size, stratify=y)

            # 超参数搜索或者固定C=10
            if search:
                params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}
                classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)
            else:
                classifier = SVC(C=10)

            # 训练模型并在测试集上进行预测
            classifier.fit(x_train, y_train)
            # accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))

            accuracies_same.append(accuracy_score(classifier.predict(x_test), classifier.predict(x_aug_test)))

            accuracies_aug.append(accuracy_score(y_test, classifier.predict(x_aug_test)))

            accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))

            # 验证集模拟：从训练集中抽取与测试集相同大小的验证集
            # val_size = len(y_test)
            # test_index = np.random.choice(range(len(x_train)), val_size, replace=False).tolist()
            # train_index = [i for i in range(len(x_train)) if i not in test_index]
            #
            # x_train_new, x_val = x_train[train_index], x_train[test_index]
            # y_train_new, y_val = y_train[train_index], y_train[test_index]
            #
            # # 重新训练并在验证集上进行预测
            # if search:
            #     classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)
            # else:
            #     classifier = SVC(C=10)
            #
            # classifier.fit(x_train_new, y_train_new)
            # accuracies_val.append(accuracy_score(y_val, classifier.predict(x_val)))

        # 计算当前测试集比例下的平均准确率
        avg_test_accuracy = np.mean(accuracies)
        # avg_val_accuracy = np.mean(accuracies_val)
        results.append((pct, avg_test_accuracy))

    # 打印每个测试集比例下的结果
    for pct, avg_test_accuracy in results:
        print(
            f"Test size: {pct}% , Avg Test Accuracy: {avg_test_accuracy:.4f} , Avg Aug Accuracy: {np.mean(accuracies_aug):.4f} , Avg Same Accuracy: {np.mean(accuracies_same):.4f}")

    return results

def randomforest_classify(x, y, search):
    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)
    accuracies = []
    accuracies_val = []
    for train_index, test_index in kf.split(x, y):

        # test
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if search:
            params = {'n_estimators': [100, 200, 500, 1000]}
            classifier = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy', verbose=0)
        else:
            classifier = RandomForestClassifier()
        classifier.fit(x_train, y_train)
        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))

        # val
        val_size = len(test_index)
        test_index = np.random.choice(test_index, val_size, replace=False).tolist()
        train_index = [i for i in train_index if not i in test_index]

        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if search:
            params = {'n_estimators': [100, 200, 500, 1000]}
            classifier = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy', verbose=0)
        else:
            classifier = RandomForestClassifier()
        classifier.fit(x_train, y_train)
        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))

    ret = np.mean(accuracies)
    return np.mean(accuracies_val), ret

def linearsvc_classify(x, y, search):
    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)
    accuracies = []
    accuracies_val = []
    for train_index, test_index in kf.split(x, y):

        # test
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if search:
            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}
            classifier = GridSearchCV(LinearSVC(), params, cv=5, scoring='accuracy', verbose=0)
        else:
            classifier = LinearSVC(C=10)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            classifier.fit(x_train, y_train)
        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))

        # val
        val_size = len(test_index)
        test_index = np.random.choice(train_index, val_size, replace=False).tolist()
        train_index = [i for i in train_index if not i in test_index]

        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if search:
            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}
            classifier = GridSearchCV(LinearSVC(), params, cv=5, scoring='accuracy', verbose=0)
        else:
            classifier = LinearSVC(C=10)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            classifier.fit(x_train, y_train)
        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))

    return np.mean(accuracies_val), np.mean(accuracies)

# def evaluate_embedding(embeddings, labels, search=True):
#
#     labels = preprocessing.LabelEncoder().fit_transform(labels)
#     x, y = np.array(embeddings), np.array(labels)
#
#     acc = 0
#     acc_val = 0
#
#     '''
#     _acc_val, _acc = logistic_classify(x, y)
#     if _acc_val > acc_val:
#         acc_val = _acc_val
#         acc = _acc
#     '''
# #
#     _acc_val, _acc = svc_classify(x,y, search)
#     if _acc_val > acc_val:
#         acc_val = _acc_val
#         acc = _acc
#
#     """
#     _acc_val, _acc = linearsvc_classify(x, y, search)
#     if _acc_val > acc_val:
#         acc_val = _acc_val
#         acc = _acc
#     """
#     '''
#     _acc_val, _acc = randomforest_classify(x, y, search)
#     if _acc_val > acc_val:
#         acc_val = _acc_val
#         acc = _acc
#     '''
#
#     print(acc_val, acc)
#
    # return acc_val, acc

def evaluate_embedding(embeddings, labels, search=True):

    labels = preprocessing.LabelEncoder().fit_transform(labels)
    x, y = np.array(embeddings), np.array(labels)
    # x_aug = np.array(emdeddings_aug)

    acc = 0
    acc_val = 0

    '''
    _acc_val, _acc = logistic_classify(x, y)
    if _acc_val > acc_val:
        acc_val = _acc_val
        acc = _acc
    '''

    # _acc_val, _acc = svc_classify_ptc(x, x_aug, y, search)
    _acc_val, _acc = svc_classify(x, y, search)
    if _acc_val > acc_val:
        acc_val = _acc_val
        acc = _acc

    # f1mis, f1mas = svc_classify(x, y, search)
    # print("f1mis: ", sum(f1mis)/len(f1mis), "std: ", np.std(f1mis))
    # print("f1mas: ", sum(f1mas)/len(f1mas), "std: ", np.std(f1mas))
    # _acc_val = np.mean(f1mis)
    # _acc = np.mean(f1mas)
    # if _acc_val > acc_val:
    #     acc_val = _acc_val
    #     acc = _acc

    # svc_classify_ptc(x, x_aug, y, search)

    """
    _acc_val, _acc = linearsvc_classify(x, y, search)
    if _acc_val > acc_val:
        acc_val = _acc_val
        acc = _acc
    """
    '''
    _acc_val, _acc = randomforest_classify(x, y, search)
    if _acc_val > acc_val:
        acc_val = _acc_val
        acc = _acc
    '''

    # print(acc_val, acc)

    return acc_val, acc

'''
if __name__ == '__main__':
    evaluate_embedding('./data', 'ENZYMES', np.load('tmp/emb.npy'))
'''
