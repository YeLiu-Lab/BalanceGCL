import argparse
import os.path as osp
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
# from core.encoders import *

# from torch_geometric.datasets import TUDataset
from aug import TUDataset_aug as TUDataset
from torch_geometric.data import DataLoader
import sys
import json
from torch import optim

from gin import EncoderGIN, EncoderGCN
from evaluate_embedding import evaluate_embedding
from model import *

# from .arguments import arg_parse
from torch_geometric.transforms import Constant
import pdb


class GcnInfomax(nn.Module):
  def __init__(self, hidden_dim, num_gc_layers, prior, dataset_num_features, alpha=0.5, beta=1., gamma=.1):
    super(GcnInfomax, self).__init__()

    self.alpha = alpha
    self.beta = beta
    self.gamma = gamma
    self.prior = prior

    self.embedding_dim = mi_units = hidden_dim * num_gc_layers
    self.encoder = EncoderGCN(dataset_num_features, hidden_dim, num_gc_layers)

    self.local_d = FF(self.embedding_dim)
    self.global_d = FF(self.embedding_dim)
    # self.local_d = MI1x1ConvNet(self.embedding_dim, mi_units)
    # self.global_d = MIFCNet(self.embedding_dim, mi_units)

    if self.prior:
        self.prior_d = PriorDiscriminator(self.embedding_dim)

    self.init_emb()

  def init_emb(self):
    initrange = -1.5 / self.embedding_dim
    for m in self.modules():
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)


  def forward(self, x, edge_index, batch, num_graphs):

    # batch_size = data.num_graphs
    if x is None:
        x = torch.ones(batch.shape[0]).to(device)

    y, M = self.encoder(x, edge_index, batch)
    
    g_enc = self.global_d(y)
    l_enc = self.local_d(M)

    mode='fd'
    measure='JSD'
    local_global_loss = local_global_loss_(l_enc, g_enc, edge_index, batch, measure)
 
    if self.prior:
        prior = torch.rand_like(y)
        term_a = torch.log(self.prior_d(prior)).mean()
        term_b = torch.log(1.0 - self.prior_d(y)).mean()
        PRIOR = - (term_a + term_b) * self.gamma
    else:
        PRIOR = 0
    
    return local_global_loss + PRIOR


class simclr(nn.Module):
  def __init__(self, hidden_dim, num_gc_layers, prior, dataset_num_features,alpha=0.5, beta=1., gamma=.1 ):
    super(simclr, self).__init__()

    self.alpha = alpha
    self.beta = beta
    self.gamma = gamma
    self.prior = prior

    self.embedding_dim = mi_units = hidden_dim * num_gc_layers  #输出维度
    self.encoder = EncoderGCN(dataset_num_features, hidden_dim, num_gc_layers)  #GIN模型

    self.proj_head = nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim), nn.ReLU(inplace=True), nn.Linear(self.embedding_dim, self.embedding_dim))

    self.init_emb()  #初始化权重

  def init_emb(self):
    initrange = -1.5 / self.embedding_dim
    for m in self.modules():
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)


  def forward(self, x, edge_index, batch, edge_weight=None):

    # batch_size = data.num_graphs
    if x is None:
        x = torch.ones(batch.shape[0]).to(device)

    y, M = self.encoder(x, edge_index, batch, edge_weight)
    
    y = self.proj_head(y)
    
    return y

  def loss_cal(self, x, x_aug):

    T = 0.2
    batch_size, _ = x.size()
    x_abs = x.norm(dim=1)
    x_aug_abs = x_aug.norm(dim=1)

    sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)
    sim_matrix = torch.exp(sim_matrix / T)
    pos_sim = sim_matrix[range(batch_size), range(batch_size)]
    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)
    loss = - torch.log(loss).mean()

    return loss



class Pretrain(nn.Module):
    def __init__(self, hidden_dim, num_gc_layers, prior, dataset_num_features, num_classes, alpha=0.5, beta=1.,
                 gamma=.1):
        super(Pretrain, self).__init__()

        # 初始化SimCLR模型
        self.simclr = simclr(hidden_dim, num_gc_layers, prior, dataset_num_features, alpha, beta, gamma)

        # 定义MLP分类器
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * num_gc_layers, hidden_dim),  # 假设embedding_dim == hidden_dim * num_gc_layers
            nn.ReLU(),
            nn.Linear(hidden_dim, num_classes)  # 输出num_classes个类别
        )

    def forward(self, x, edge_index, batch, edge_weight=None):
        # 使用SimCLR模型获取表示
        for param in self.simclr.parameters():
            param.requires_grad = False
        embeddings = self.simclr(x, edge_index, batch, edge_weight)

        # 将表示通过MLP分类器
        logits = self.classifier(embeddings)

        return logits, embeddings

    # 如果需要计算损失，可以在这里定义或直接使用外部定义的损失函数


import random
def setup_seed(seed):

    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    np.random.seed(seed)
    random.seed(seed)


def run(epochs, patience, lr, weight_decay, dataset):
    ap = argparse.ArgumentParser(description='GraphCL')
    ap.add_argument('--DS', type=str, default='ENZYMES')
    ap.add_argument('--local', action='store_true', default=False)
    ap.add_argument('--lr', type=float, default=0.01)
    ap.add_argument('--num_gc_layers', type=int, default=5)
    ap.add_argument('--aug', type=str, default='dnodes')
    ap.add_argument('--seed', type=int, default=0)
    ap.add_argument('--hidden_dim', type=int, default=64)
    ap.add_argument('--prior', action='store_true', default=False)
    ap.add_argument('--num_epochs', type=int, default=400)

    args = ap.parse_args()
    setup_seed(args.seed)

    accuracies = {'val':[], 'test':[]}
    epochs = epochs
    log_interval = 10
    batch_size = 128
    # batch_size = 512
    lr = lr
    DS = dataset
    prior = args.prior


    path = osp.join(osp.dirname(osp.realpath(__file__)), '.', 'data', DS)
    # kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)

    dataset = TUDataset(path, name=DS, aug=args.aug).shuffle()
    dataset_eval = TUDataset(path, name=DS, aug='none').shuffle()
    print(len(dataset))
    print(dataset.get_num_feature())
    try:
        dataset_num_features = dataset.get_num_feature()  #节点特征维度
    except:
        dataset_num_features = 1

    dataloader = DataLoader(dataset, batch_size=batch_size)
    dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size)

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    # model = simclr(args.hidden_dim, args.num_gc_layers, prior, dataset_num_features).to(device)
    model = Pretrain(args.hidden_dim, args.num_gc_layers, prior, dataset_num_features, dataset.num_classes).to(device)
    # print(model)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    print('================')
    print('lr: {}'.format(lr))
    print('num_features: {}'.format(dataset_num_features))
    print('hidden_dim: {}'.format(args.hidden_dim))
    print('num_gc_layers: {}'.format(args.num_gc_layers))
    print('================')

    model.eval()
    emb, y = model.simclr.encoder.get_embeddings(dataloader_eval)  #输出维度为hidden_dim * num_gc_layers，1536
    # print(emb.shape, y.shape)

    """
    acc_val, acc = evaluate_embedding(emb, y)
    accuracies['val'].append(acc_val)
    accuracies['test'].append(acc)
    """

    minest_loss = 1000
    early_stop = 0
    save_path = f'models/model_{args.DS}_{args.num_gc_layers}_layers_{args.hidden_dim}_dim_{lr}_lr.pth'

    best_val_acc = 0
    best_acc = 0
    for epoch in range(1, epochs+1):
        loss_all = 0
        model.train()
        early_stop += 1
        for data in dataloader:

            # print('start')
            data, data_aug = data
            optimizer.zero_grad()

            
            node_num, _ = data.x.size()
            data = data.to(device)
            x = model(data.x, data.edge_index, data.batch)[1]  #输出维度为hidden_dim * num_gc_layers，1536

            data_aug = data_aug.to(device)

            x_aug = model(data_aug.x, data_aug.edge_index, data_aug.batch)[1]


            loss = model.simclr.loss_cal(x, x_aug)

            loss.backward()
            optimizer.step()
            # print(loss)
            loss_all += loss.item() * data.num_graphs
        print('Epoch {}, Loss {}'.format(epoch, loss_all / len(dataloader)))

        if epoch % 20 == 0:
            model.eval()
            emb, y = model.simclr.encoder.get_embeddings(dataloader_eval)
            acc_val, acc = evaluate_embedding(emb, y)
            print(acc_val, acc)
            if acc_val > best_val_acc:
                best_val_acc = acc_val
                best_acc = acc
                torch.save(model.state_dict(), save_path)

    print('best_val_acc:', best_val_acc, 'best_acc:', best_acc)



    return best_acc

if __name__ == "__main__":
    # 设置随机种子
    setup_seed(42)

    # 定义参数
    epochs = 400
    patience = 50
    lr = 0.01
    weight_decay = 1e-4
    dataset = 'ENZYMES'  # 示例数据集名称

    # 调用run函数并获取结果
    best_accuracy = run(epochs, patience, lr, weight_decay, dataset)
    print(f"Best accuracy achieved: {best_accuracy:.4f}")
