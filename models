from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN

from torch_geometric.nn import global_mean_pool, GINConv, global_add_pool, GCNConv
from dgl.nn.pytorch.conv import GINConv as dgl_GINConv

import torch
import torch.nn.functional as F
from torch.nn import Linear
import numpy as np
import torch.nn as nn


class Encoder(torch.nn.Module):
    def __init__(self, num_features, dim, num_gc_layers):
        super(Encoder, self).__init__()


        self.num_gc_layers = num_gc_layers

        self.convs = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()

        for i in range(num_gc_layers):

            if i:
                nn = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))
            else:
                nn = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))
            conv = GINConv(nn)
            bn = torch.nn.BatchNorm1d(dim)

            self.convs.append(conv)
            self.bns.append(bn)

    def forward(self, x, edge_index, batch):

        if x is None:
            x = torch.ones((batch.shape[0], 1)).to(device)

        xs = []
        for i in range(self.num_gc_layers):

            x = F.relu(self.convs[i](x, edge_index))
            x = self.bns[i](x)
            xs.append(x)


        xpool = [global_add_pool(x, batch) for x in xs]
        x = torch.cat(xpool, 1)

        return x, torch.cat(xs, 1)

    def get_embeddings(self, loader):

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        ret = []
        y = []
        with torch.no_grad():
            for data in loader:

                data.to(device)
                x, edge_index, batch = data.x, data.edge_index, data.batch
                if x is None:
                    x = torch.ones((batch.shape[0],1)).to(device)
                x, _ = self.forward(x, edge_index, batch)

                ret.append(x.cpu().numpy())
                y.append(data.y.cpu().numpy())
        ret = np.concatenate(ret, 0)
        y = np.concatenate(y, 0)
        return ret, y

    def get_embeddings_v(self, loader):

        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        ret = []
        y = []
        with torch.no_grad():
            for n, data in enumerate(loader):
                data.to(device)
                x, edge_index, batch = data.x, data.edge_index, data.batch
                if x is None:
                    x = torch.ones((batch.shape[0],1)).to(device)
                x_g, x = self.forward(x, edge_index, batch)
                x_g = x_g.cpu().numpy()
                ret = x.cpu().numpy()
                y = data.edge_index.cpu().numpy()
                print(data.y)
                if n == 1:
                   break

        return x_g, ret, y


class simclr(nn.Module):
    def __init__(self, hidden_dim, num_gc_layers, prior, dataset_num_features, alpha=0.5, beta=1., gamma=.1):
        super(simclr, self).__init__()

        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.prior = prior

        self.embedding_dim = mi_units = hidden_dim * num_gc_layers
        self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)

        self.proj_head = nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim), nn.ReLU(inplace=True),
                                       nn.Linear(self.embedding_dim, self.embedding_dim))

        self.init_emb()

    def init_emb(self):
        initrange = -1.5 / self.embedding_dim
        for m in self.modules():
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight.data)
                if m.bias is not None:
                    m.bias.data.fill_(0.0)

    def forward(self, x, edge_index, batch, num_graphs):

        if x is None:
            x = torch.ones(batch.shape[0]).to(device)

        y, M = self.encoder(x, edge_index, batch)

        y = self.proj_head(y)

        return y

    def loss_cal(self, x, x_positive, x_negatives):
        T = 0.2
        batch_size, _ = x.size()
        x_abs = x.norm(dim=1)
        x_positive_abs = x_positive.norm(dim=1)
        x_negatives_abs = x_negatives.norm(dim=2)

        sim_matrix_pos = torch.einsum('ik,jk->ij', x, x_positive) / torch.einsum('i,j->ij', x_abs, x_positive_abs)
        sim_matrix_pos = torch.exp(sim_matrix_pos / T)

        dot_product = torch.einsum('ik,ljk->lij', x, x_negatives)
        norm_product = torch.einsum('i,jk->jik', x_abs, x_negatives_abs)

        sim_matrix_neg = dot_product / norm_product

        pos_sim = sim_matrix_pos.diag()

        neg_sim_sum = sim_matrix_neg.sum(dim=0).diag()

        loss = pos_sim / neg_sim_sum
        loss = - torch.log(loss).mean()

        return loss

def loss_cal(x, x_positive, x_negatives, T):
    batch_size, _ = x.size()
    x_abs = x.norm(dim=1)
    x_positive_abs = x_positive.norm(dim=1)
    x_negatives_abs = x_negatives.norm(dim=2)

    sim_matrix_pos = torch.einsum('ik,jk->ij', x, x_positive) / torch.einsum('i,j->ij', x_abs, x_positive_abs)
    sim_matrix_pos = torch.exp(sim_matrix_pos / T)

    dot_product = torch.einsum('ik,ljk->lij', x, x_negatives)
    norm_product = torch.einsum('i,jk->jik', x_abs, x_negatives_abs)

    sim_matrix_neg = dot_product / norm_product
    sim_matrix_neg = torch.exp(sim_matrix_neg / T)

    pos_sim = sim_matrix_pos.diag()

    neg_sim_sum = sim_matrix_neg.sum(dim=0).diag()

    loss = pos_sim / neg_sim_sum
    loss = - torch.log(loss).mean()

    return loss
